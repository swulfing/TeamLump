Tow_Times <- Trawl_Tow %>%
select(Season, Year, Tow_Number, Tow_Time)
No_Caught <- Trawl_Catch %>%
select(Season, Year, Tow_Number, Number_Caught)
# Transformed frequency back into total number caught
Trawl_Length_combine <- left_join(Trawl_Length, No_Caught,
by = c("Season", "Year", "Tow_Number")) %>%
left_join(., Tow_Times,
by = c("Season", "Year", "Tow_Number"))
Bio_check <- data.frame(read.csv("ME_DMR/Lumpfish_Bio_check.csv"))
Trawl_Length <- data.frame(read.csv("ME_DMR/MaineDMR_Trawl_Survey_Catch_at_Length_Data_2022.05.21.csv"))
Trawl_Catch <- data.frame(read.csv("ME_DMR/MaineDMR_Trawl_Survey_Catch_Data_2022.05.21.csv"))
Trawl_Tow <- data.frame(read.csv("ME_DMR/MaineDMR_Trawl_Survey_Tow_Data_2022.05.21.csv"))
#Clean tow dataset - fix dates, average lat/lon data and depths. Make depth in meters
Trawl_Tow$Date <- as.Date(Trawl_Tow$Start_Date)
Trawl_Tow$Start_Date <- as.POSIXct(Trawl_Tow$Start_Date, format = "%Y-%m-%dT%H:%M:00Z")
Trawl_Tow$Time <- format(as.POSIXct(Trawl_Tow$Start_Date), format =  "%H:%M")
Trawl_Tow_clean <- Trawl_Tow %>%
rowwise() %>%
mutate(Avg_Depth_meters = mean(c(Start_Depth_fathoms, End_Depth_fathoms), na.rm = TRUE) * 1.8288) %>%
mutate(Average_Lat = mean(c(Start_Latitude, End_Latitude))) %>%
mutate(Average_Lon = mean(c(Start_Longitude, End_Longitude)))
Trawl_Catch_clean <- Trawl_Catch %>%
mutate(WEIGHT_AVG = Weight_kg/Number_Caught) %>%
select("Season", "Year", "Tow_Number", "WEIGHT_AVG") %>%
group_by(Season, Year, Tow_Number)
# #Clean length dataset - Need to de-standardize frequency back into total number caught
# #Add tow time (from tow dataset) and no caught (from catch dataset) to length dataset to figure out how many actual fish at each length
Tow_Times <- Trawl_Tow %>%
select(Season, Year, Tow_Number, Tow_Time)
No_Caught <- Trawl_Catch %>%
select(Season, Year, Tow_Number, Number_Caught)
# Transformed frequency back into total number caught
Trawl_Length_combine <- left_join(Trawl_Length, No_Caught,
by = c("Season", "Year", "Tow_Number")) %>%
left_join(., Tow_Times,
by = c("Season", "Year", "Tow_Number"))
Trawl_Length_combine <- Trawl_Length_combine %>%
rowwise() %>%
mutate(Tow_Secs = as.numeric(substring(Tow_Time, regexpr(":", Tow_Time)+1))) %>%
mutate(No_at_Length = (Frequency * Tow_Secs) / 20)
#Creating one row per lumpfish caugth
Trawl_Length_clean <- Trawl_Length_combine %>% #as.data.frame(lapply(Trawl_Length_combine, rep, Trawl_Length_combine$No_at_Length)) %>%
select(Season, Year, Tow_Number, Length, No_at_Length) %>%
group_by(Season, Year, Tow_Number) %>%
mutate(positionInCategory = 1:n()) #Numbers each fish within a tow to match up with lengths later
ME_DMR_catch <- left_join(Trawl_Catch_clean, Trawl_Length_clean,
by = c("Season", "Year", "Tow_Number", "positionInCategory")) %>%
select(Season, Year, Tow_Number, WEIGHT_AVG, Length)
View(Trawl_Catch)
View(Trawl_Catch_clean)
View(Trawl_Length_combine)
View(Trawl_Length_combine)
View(Trawl_Length_clean)
ME_DMR_catch <- left_join(Trawl_Catch_clean, Trawl_Length_clean,
by = c("Season", "Year", "Tow_Number")) %>%
select(Season, Year, Tow_Number, WEIGHT_AVG, Length)
#Combine with trawl data, Rename columns, Add Dataset ID
ME_DMR_clean <- right_join(Trawl_Tow_clean, ME_DMR_catch,
by = c("Season", "Year", "Tow_Number")) %>%
select(Survey,
Season,
Date,
#Time,
Air_Temp,
Bottom_WaterTemp_DegC,
Bottom_Salinity,
Surface_WaterTemp_DegC,
Surface_Salinity,
Avg_Depth_meters,
Average_Lat,
Average_Lon,
WEIGHT_AVG,
Length) %>%
rename(LON = Average_Lon,
LAT = Average_Lat,
DATE = Date,
#TIME = Time,
SEASON = Season,
SURVEY_ID = Survey,
DEPTH = Avg_Depth_meters,
TEMP_AIR = Air_Temp,
TEMP_BOTTOM = Bottom_WaterTemp_DegC,
TEMP_SURFACE = Surface_WaterTemp_DegC,
SALINITY_BOTTOM = Bottom_Salinity, #CHECK UNITS. PPT?
SALINITY_SURFACE = Surface_Salinity, #CHECK UNITS. PPT?
LENGTH = Length) %>%
mutate(SEASON = replace(SEASON, SEASON == "Spring", "SPRING")) %>%
mutate(SEASON = replace(SEASON, SEASON == "Fall", "FALL")) %>%
mutate(DATA_SOURCE = "ME_DMR") %>%
mutate(NOTES = "Depth, LAT, and LON taken as average of start and end of tow measurements")
View(ME_DMR_catch)
ME_DMR_catch <- left_join(Trawl_Catch_clean, Trawl_Length_clean,
by = c("Season", "Year", "Tow_Number")) %>%
select(Season, Year, Tow_Number, WEIGHT_AVG, Length, No_at_Length)
#Combine with trawl data, Rename columns, Add Dataset ID
ME_DMR_clean <- right_join(Trawl_Tow_clean, ME_DMR_catch,
by = c("Season", "Year", "Tow_Number")) %>%
select(Survey,
Season,
Date,
#Time,
Air_Temp,
Bottom_WaterTemp_DegC,
Bottom_Salinity,
Surface_WaterTemp_DegC,
Surface_Salinity,
Avg_Depth_meters,
Average_Lat,
Average_Lon,
WEIGHT_AVG,
Length) %>%
rename(LON = Average_Lon,
LAT = Average_Lat,
DATE = Date,
#TIME = Time,
SEASON = Season,
SURVEY_ID = Survey,
DEPTH = Avg_Depth_meters,
TEMP_AIR = Air_Temp,
TEMP_BOTTOM = Bottom_WaterTemp_DegC,
TEMP_SURFACE = Surface_WaterTemp_DegC,
SALINITY_BOTTOM = Bottom_Salinity, #CHECK UNITS. PPT?
SALINITY_SURFACE = Surface_Salinity, #CHECK UNITS. PPT?
LENGTH = Length) %>%
mutate(SEASON = replace(SEASON, SEASON == "Spring", "SPRING")) %>%
mutate(SEASON = replace(SEASON, SEASON == "Fall", "FALL")) %>%
mutate(DATA_SOURCE = "ME_DMR") %>%
mutate(NOTES = "Depth, LAT, and LON taken as average of start and end of tow measurements")
#Combine with trawl data, Rename columns, Add Dataset ID
ME_DMR_clean <- right_join(Trawl_Tow_clean, ME_DMR_catch,
by = c("Season", "Year", "Tow_Number")) %>%
select(Survey,
Season,
Date,
#Time,
Air_Temp,
Bottom_WaterTemp_DegC,
Bottom_Salinity,
Surface_WaterTemp_DegC,
Surface_Salinity,
Avg_Depth_meters,
Average_Lat,
Average_Lon,
No_at_Length,
WEIGHT_AVG,
Length) %>%
rename(LON = Average_Lon,
LAT = Average_Lat,
DATE = Date,
#TIME = Time,
SEASON = Season,
SURVEY_ID = Survey,
DEPTH = Avg_Depth_meters,
TEMP_AIR = Air_Temp,
TEMP_BOTTOM = Bottom_WaterTemp_DegC,
TEMP_SURFACE = Surface_WaterTemp_DegC,
SALINITY_BOTTOM = Bottom_Salinity, #CHECK UNITS. PPT?
SALINITY_SURFACE = Surface_Salinity, #CHECK UNITS. PPT?
NO_TOW = No_at_Length,
LENGTH = Length) %>%
mutate(SEASON = replace(SEASON, SEASON == "Spring", "SPRING")) %>%
mutate(SEASON = replace(SEASON, SEASON == "Fall", "FALL")) %>%
mutate(DATA_SOURCE = "ME_DMR") %>%
mutate(NOTES = "Depth, LAT, and LON taken as average of start and end of tow measurements")
rm(list=setdiff(ls(), datasets))
install.packages("sf")
knitr::opts_chunk$set(echo = FALSE,warning = FALSE, message = FALSE, tidy.opts=list(width.cutoff=60),tidy=TRUE, cache = TRUE, dev = c('pdf','png'), dpi=300)
library(tidyverse)
library(sf)
library(mapview)
install.packages("mapview")
library(lubridate)
library(gganimate)
library(gifski)
library(maps)
library(knitr)
library(ggpubr)
library(magick)
library(ggplot2)
library(dplyr)
install.packages(c("gganimate", "gifski", "magick"))
dist_data <- Lumpfish_Data_map
#filter(LAT >= 41.5) %>%
#filter(DATA_SOURCE == "NOAA_Observer")
ggplot(dist_data, aes(LON, LAT)) +
theme_qmel() +
geom_hex(bins = 100) +
geom_polygon(data = county_info, mapping = aes(x = long, y = lat, group = group), color = "black", fill = "white") +
#geom_path(data = county_info, mapping = aes(x = long, y = lat, group = group)) +
coord_quickmap(xlim = c(min_long, max_long),  ylim = c(min_lat, max_lat)) +
xlab("Longitude") +
ylab("Latitude") +
scale_fill_continuous(name = "Lumpfish Count")
knitr::opts_chunk$set(echo = FALSE,warning = FALSE, message = FALSE, tidy.opts=list(width.cutoff=60),tidy=TRUE, cache = TRUE, dev = c('pdf','png'), dpi=300)
library(tidyverse)
library(sf)
library(mapview)
library(lubridate)
library(gganimate)
library(gifski)
library(maps)
library(knitr)
library(ggpubr)
library(magick)
library(ggplot2)
library(dplyr)
theme_qmel <- function(){
theme_classic() + theme(panel.border = element_blank(),panel.grid.major = element_blank(),panel.grid.minor = element_blank(),axis.text=element_text(size=8),
axis.title=element_text(size=14))
}
#setwd("C:/Users/sophi/Documents/GitHub/TeamLump")
Lumpfish_Data_clean <- read.csv("DatatoClean/Lumpfish_Data_clean.csv")
Lumpfish_Data_clean$YEAR <- year(Lumpfish_Data_clean$DATE)
Lumpfish_Data_occurrences <- read.csv("DatatoClean/Lumpfish_Data_occurrences.csv")
Lumpfish_Data_occurrences$YEAR <- year(Lumpfish_Data_occurrences$DATE)
Lumpfish_Data_map <- Lumpfish_Data_clean %>%
drop_na(c("LAT", "LON", "DATE", "YEAR")) %>%
mutate(across(SEASON, factor, levels = c("WINTER", "SPRING", "SUMMER", "FALL")))
which_state <- c("maine", "new hampshire", "vermont", "massachusetts", "rhode island", "connecticut",
"new york", "new jersey", "pennsylvania", "virginia", "maryland", "washington dc", "delaware")
county_info <- map_data("state", region=which_state)
base_map <- ggplot(data = county_info, mapping = aes(x = long, y = lat, group = group)) +
geom_polygon(color = "black", fill = "white") +
coord_quickmap() +
theme_void()
min_long <- min(Lumpfish_Data_map$LON)
max_long <- -66 #max(Lumpfish_Data_map$LON)
min_lat <- 41.5 #min(Lumpfish_Data_map$LAT)
max_lat <- max(Lumpfish_Data_map$LAT)
zoom_map <- base_map +
coord_quickmap(xlim = c(min_long, max_long),  ylim = c(min_lat, max_lat))
# testing <- Lumpfish_Data_clean %>%
#   filter(DATA_SOURCE == "NEFSC") %>%
#   #drop_na(c("LAT", "LON", "DATE", "YEAR")) %>%
#   filter(is.na(YEAR))
#
#
# View(testing)
#
#
# min(testing$YEAR)
dist_data <- Lumpfish_Data_map
#filter(LAT >= 41.5) %>%
#filter(DATA_SOURCE == "NOAA_Observer")
ggplot(dist_data, aes(LON, LAT)) +
theme_qmel() +
geom_hex(bins = 100) +
geom_polygon(data = county_info, mapping = aes(x = long, y = lat, group = group), color = "black", fill = "white") +
#geom_path(data = county_info, mapping = aes(x = long, y = lat, group = group)) +
coord_quickmap(xlim = c(min_long, max_long),  ylim = c(min_lat, max_lat)) +
xlab("Longitude") +
ylab("Latitude") +
scale_fill_continuous(name = "Lumpfish Count")
install.packages("hexbin")
knitr::opts_chunk$set(echo = FALSE,warning = FALSE, message = FALSE, tidy.opts=list(width.cutoff=60),tidy=TRUE, cache = TRUE, dev = c('pdf','png'), dpi=300)
library(tidyverse)
library(sf)
library(mapview)
library(lubridate)
library(gganimate)
library(gifski)
library(maps)
library(knitr)
library(ggpubr)
library(magick)
library(ggplot2)
library(dplyr)
library(hexbin)
theme_qmel <- function(){
theme_classic() + theme(panel.border = element_blank(),panel.grid.major = element_blank(),panel.grid.minor = element_blank(),axis.text=element_text(size=8),
axis.title=element_text(size=14))
}
#setwd("C:/Users/sophi/Documents/GitHub/TeamLump")
Lumpfish_Data_clean <- read.csv("DatatoClean/Lumpfish_Data_clean.csv")
Lumpfish_Data_clean$YEAR <- year(Lumpfish_Data_clean$DATE)
Lumpfish_Data_occurrences <- read.csv("DatatoClean/Lumpfish_Data_occurrences.csv")
Lumpfish_Data_occurrences$YEAR <- year(Lumpfish_Data_occurrences$DATE)
Lumpfish_Data_map <- Lumpfish_Data_clean %>%
drop_na(c("LAT", "LON", "DATE", "YEAR")) %>%
mutate(across(SEASON, factor, levels = c("WINTER", "SPRING", "SUMMER", "FALL")))
which_state <- c("maine", "new hampshire", "vermont", "massachusetts", "rhode island", "connecticut",
"new york", "new jersey", "pennsylvania", "virginia", "maryland", "washington dc", "delaware")
county_info <- map_data("state", region=which_state)
base_map <- ggplot(data = county_info, mapping = aes(x = long, y = lat, group = group)) +
geom_polygon(color = "black", fill = "white") +
coord_quickmap() +
theme_void()
min_long <- min(Lumpfish_Data_map$LON)
max_long <- -66 #max(Lumpfish_Data_map$LON)
min_lat <- 41.5 #min(Lumpfish_Data_map$LAT)
max_lat <- max(Lumpfish_Data_map$LAT)
zoom_map <- base_map +
coord_quickmap(xlim = c(min_long, max_long),  ylim = c(min_lat, max_lat))
# testing <- Lumpfish_Data_clean %>%
#   filter(DATA_SOURCE == "NEFSC") %>%
#   #drop_na(c("LAT", "LON", "DATE", "YEAR")) %>%
#   filter(is.na(YEAR))
#
#
# View(testing)
#
#
# min(testing$YEAR)
# testing <- Lumpfish_Data_clean %>%
#   filter(DATA_SOURCE == "NEFSC") %>%
#   #drop_na(c("LAT", "LON", "DATE", "YEAR")) %>%
#   filter(is.na(YEAR))
#
#
# View(testing)
#
#
# min(testing$YEAR)
temptest <- Lumpfish_Data_clean %>%
group_by(DATA_SOURCE) %>%
summarize(mean(TEMP_BOTTOM))
View(temptest)
# testing <- Lumpfish_Data_clean %>%
#   filter(DATA_SOURCE == "NEFSC") %>%
#   #drop_na(c("LAT", "LON", "DATE", "YEAR")) %>%
#   filter(is.na(YEAR))
#
#
# View(testing)
#
#
# min(testing$YEAR)
temptest <- Lumpfish_Data_clean %>%
group_by(DATA_SOURCE) %>%
summarize(mean(TEMP_BOTTOM), na.rm = TRUE)
View(temptest)
# testing <- Lumpfish_Data_clean %>%
#   filter(DATA_SOURCE == "NEFSC") %>%
#   #drop_na(c("LAT", "LON", "DATE", "YEAR")) %>%
#   filter(is.na(YEAR))
#
#
# View(testing)
#
#
# min(testing$YEAR)
temptest <- Lumpfish_Data_clean %>%
group_by(DATA_SOURCE) %>%
summarize(mean(TEMP_BOTTOM, na.rm = TRUE))
View(temptest)
knitr::opts_chunk$set(echo = FALSE,warning = FALSE, message = FALSE, tidy.opts=list(width.cutoff=60),tidy=TRUE, cache = TRUE, dev = c('pdf','png'), dpi=300)
library(tidyverse)
library(sf)
library(mapview)
library(lubridate)
library(gganimate)
library(gifski)
library(maps)
library(knitr)
library(ggpubr)
library(magick)
library(ggplot2)
library(dplyr)
library(hexbin)
theme_qmel <- function(){
theme_classic() + theme(panel.border = element_blank(),panel.grid.major = element_blank(),panel.grid.minor = element_blank(),axis.text=element_text(size=8),
axis.title=element_text(size=14))
}
#setwd("C:/Users/sophi/Documents/GitHub/TeamLump")
Lumpfish_Data_clean <- read.csv("DatatoClean/Lumpfish_Data_clean.csv")
Lumpfish_Data_clean$YEAR <- year(Lumpfish_Data_clean$DATE)
Lumpfish_Data_occurrences <- read.csv("DatatoClean/Lumpfish_Data_occurrences.csv")
Lumpfish_Data_occurrences$YEAR <- year(Lumpfish_Data_occurrences$DATE)
Lumpfish_Data_map <- Lumpfish_Data_clean %>%
drop_na(c("LAT", "LON", "DATE", "YEAR")) %>%
mutate(across(SEASON, factor, levels = c("WINTER", "SPRING", "SUMMER", "FALL")))
which_state <- c("maine", "new hampshire", "vermont", "massachusetts", "rhode island", "connecticut",
"new york", "new jersey", "pennsylvania", "virginia", "maryland", "washington dc", "delaware")
county_info <- map_data("state", region=which_state)
base_map <- ggplot(data = county_info, mapping = aes(x = long, y = lat, group = group)) +
geom_polygon(color = "black", fill = "white") +
coord_quickmap() +
theme_void()
min_long <- min(Lumpfish_Data_map$LON)
max_long <- -66 #max(Lumpfish_Data_map$LON)
min_lat <- 41.5 #min(Lumpfish_Data_map$LAT)
max_lat <- max(Lumpfish_Data_map$LAT)
zoom_map <- base_map +
coord_quickmap(xlim = c(min_long, max_long),  ylim = c(min_lat, max_lat))
# testing <- Lumpfish_Data_clean %>%
#   filter(DATA_SOURCE == "NEFSC") %>%
#   #drop_na(c("LAT", "LON", "DATE", "YEAR")) %>%
#   filter(is.na(YEAR))
#
#
# View(testing)
#
#
# min(testing$YEAR)
# temptest <- Lumpfish_Data_clean %>%
#   group_by(DATA_SOURCE) %>%
#   summarize(mean(TEMP_BOTTOM, na.rm = TRUE))
#
# View(temptest)
min(Lumpfish_Data_clean$TEMP_BOTTOM)
class(Lumpfish_Data_clean$TEMP_BOTTOM)
max(Lumpfish_Data_clean$TEMP_BOTTOM)
#setwd("C:/Users/sophi/Documents/GitHub/TeamLump")
Lumpfish_Data_clean <- read.csv("DatatoClean/Lumpfish_Data_clean.csv")
Lumpfish_Data_clean$YEAR <- year(Lumpfish_Data_clean$DATE)
max(Lumpfish_Data_clean$TEMP_BOTTOM)
max(Lumpfish_Data_clean$YEAR)
head(Lumpfish_Data_clean)
class(Lumpfish_Data_clean$TEMP_BOTTOM)
min(Lumpfish_Data_clean$TEMP_BOTTOM)
max(Lumpfish_Data_clean$TEMP_BOTTOM)
plot(Lumpfish_Data_clean$TEMP_BOTTOM)
temptest <- Lumpfish_Data_clean %>%
drop_na(c("TEMP_BOTTOM"))
min(temptest$TEMP_BOTTOM)
max(temptest$TEMP_BOTTOM)
head(zoom_map)
head(Lumpfish_Data_map)
depth_graph <- Lumpfish_Data_map %>%
filter(!is.na(DEPTH))
depthMap_zoom <- zoom_map +
geom_point(data = depth_graph, aes(x = LON, y = LAT, group=DEPTH, colour = DEPTH)) +
scale_colour_continuous(name = "Depth (m)") +
xlab("Longitude") +
ylab("Latitude") +
theme_qmel()
depthMap_zoom
depth_graph <- Lumpfish_Data_map %>%
filter(!is.na(DEPTH))
depthMap_zoom <- zoom_map +
geom_point(data = depth_graph, aes(x = LON, y = LAT, group=DEPTH, colour = DEPTH)) +
scale_colour_continuous(name = "Depth (m)") +
xlab("Longitude") +
ylab("Latitude") +
theme_qmel()
depthMap_zoom
min(depth_graph$DEPTH)
max(depth_graph$DEPTH)
temp_graph <- Lumpfish_Data_map %>%
filter(!is.na(TEMP_BOTTOM))
tempMap_zoom <- zoom_map +
geom_point(data = temp_graph, aes(x = LON, y = LAT, group=TEMP_BOTTOM, colour = TEMP_BOTTOM)) +
scale_colour_continuous(name = "Bottom Temperature (c)") +
xlab("Longitude") +
ylab("Latitude") +
theme_qmel()
tempMap_zoom
plot(depth_graph$TEMP_BOTTOM~depth_graph$DEPTH)
library(tidyverse)
library(sf)
library(mapview)
install.packages("mapview")
library(mapview)
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
devtools::install('/tmp/htmltools')
devtools::install_github('rstudio/htmltools')
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(sf)
library(mapview)
library(lubridate)
library(gganimate)
library(gifski)
library(maps)
library(knitr)
library(ggpubr)
library(magick)
library(ggplot2)
library(dplyr)
setwd("C:/Users/sophi/Documents/GitHub/TeamLump")
Lumpfish_Data_clean <- read.csv("DatatoClean/Lumpfish_Data_clean.csv")
Lumpfish_Data_clean$YEAR <- year(Lumpfish_Data_clean$DATE)
Lumpfish_Data_map <- Lumpfish_Data_clean %>%
drop_na(c("LAT", "LON", "DATE", "YEAR")) %>%
mutate(across(SEASON, factor, levels = c("WINTER", "SPRING", "SUMMER", "FALL")))
which_state <- c("maine", "new hampshire", "vermont", "massachusetts", "rhode island", "connecticut",
"new york", "new jersey", "pennsylvania", "virginia", "maryland", "washington dc", "delaware")
county_info <- map_data("state", region=which_state)
base_map <- ggplot(data = county_info, mapping = aes(x = long, y = lat, group = group)) +
geom_polygon(color = "black", fill = "white") +
coord_quickmap() +
theme_void()
min_long <- min(Lumpfish_Data_map$LON)
max_long <- max(Lumpfish_Data_map$LON)
min_lat <- 41.5 #min(Lumpfish_Data_map$LAT)
max_lat <- max(Lumpfish_Data_map$LAT)
zoom_map <- base_map +
coord_quickmap(xlim = c(min_long, max_long),  ylim = c(min_lat, max_lat))
# testing <- Lumpfish_Data_clean %>%
#   filter(DATA_SOURCE == "NEFSC") %>%
#   #drop_na(c("LAT", "LON", "DATE", "YEAR")) %>%
#   filter(is.na(YEAR))
#
#
# View(testing)
#
#
# min(testing$YEAR)
nrow(Lumpfish_Data_clean)
nrow(Lumpfish_Data_map)
test <- Lumpfish_Data_clean %>%
filter(YEAR <= 2021)
nrow(test)
1357 + 104 + 120 + 649 + 9912
